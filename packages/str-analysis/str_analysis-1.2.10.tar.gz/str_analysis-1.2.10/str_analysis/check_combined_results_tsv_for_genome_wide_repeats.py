"""
The script takes a .tsv file generated by the combine_str_json_to_tsv.py script for ExpansionHunter calls at
genome-wide loci.
It then prints out the subset of rows that might represent pathogenic expansions and should be reviewed in more detail.

The input .tsv must have at least these input columns:

    "LocusId", "SampleId", "Sample_affected", "Sample_sex",
    "Num Repeats: Allele 1", "Num Repeats: Allele 2", "CI end: Allele 1", "CI end: Allele 2",

One way to add the  Sample_* and VariantCatalog_* columns is to run combine_str_json_to_tsv.py with the
--sample-metadata and  the --variant-catalog args. Variant catalogs can be taken from the variant_catalogs directory in
this github repo.
"""

import argparse
import os
import subprocess
from ast import literal_eval

import pandas as pd
from str_analysis.utils.canonical_repeat_unit import compute_canonical_motif
from tabulate import tabulate


REQUIRED_COLUMNS = [
    "LocusId",
    "SampleId",
    "Sample_affected",
    "Sample_sex",
    "Num Repeats: Allele 1",
    "Num Repeats: Allele 2",
    "CI end: Allele 1",
    "CI end: Allele 2",
]

OPTIONAL_COLUMNS = [
    "RepeatUnit",
    "VariantId",

    "Sample_analysis_status",
    "Sample_coded_phenotype",
    "Sample_phenotypes",
    "Sample_genome_version",

    "VariantCatalog_Inheritance",
    "VariantCatalog_Diseases",

    "Genotype",
    "GenotypeConfidenceInterval",
]

SEPARATOR_STRING = "---"

def run(command):
    print(command)
    subprocess.check_call(command, shell=True)


def parse_args():
    p = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    p.add_argument("-n", "--max-rows", type=int, default=10000000, help="Limit the max number of samples to include in "
        "the output for each locus.")
    p.add_argument("--max-n-unaffected", type=int, default=10, help="After this many unaffected samples are "
        "encountered with genotypes above a particular expansion size at locus, all samples (affected or unaffected) "
        "that have smaller expansions at that locus will be ignored")

    p.add_argument("-l", "--locus", action="append", help="If specified, only these locus ids will be processed")
    p.add_argument("--highlight-samples", nargs="*", help="If specified, this can be the path of a text file that "
        "contains sample ids (one per line) or just 1 or more sample ids listed on the commandline")
    p.add_argument("--truth-samples", help="If specified, this can be the path of a table that contains two columns: "
        "sample id and locus id. The script will then mark each sample at that locus to indicate that it's a truth "
        "sample")
    p.add_argument("--previously-seen-samples", help="If specified, this can be the path of a table that contains "
        "two columns: sample id and locus id. The script will then mark each sample at that locus to "
        "indicate that it's already been evaluated")
    p.add_argument("--previously-diagnosed-loci", nargs="*", help="If specified, this can be the path of a "
        "text file that contains locus ids (one per line) or just 1 or more locus ids listed on the commandline. These "
        "loci will be marked as having been previously diagnosed using short-read sequencing data.")
    p.add_argument("combined_tsv_path", nargs="+", help="Path of combined ExpansionHunter .tsv table generated by the "
        "combine_str_json_to_tsv.py script. It's assumed that combine_str_json_to_tsv.py "
        "was run with --sample-metadata and --variant-catalog args to add sample-level and locus-level metadata columns")

    p.add_argument("--results-path", help="Export a .tsv table of rows that pass thresholds to this path",
        default="pathogenic_and_intermediate_results.tsv")

    # Parse and validate command-line args + read in the combined table(s) from the given command_tsv_path(s)
    args = p.parse_args()

    for combined_tsv_path in args.combined_tsv_path:
        if not os.path.isfile(combined_tsv_path):
            p.error(f"{combined_tsv_path} not found")

    return args



def load_results_tables(args):
    all_dfs = []
    for combined_tsv_path in args.combined_tsv_path:

        # read in table
        print(f"Reading {combined_tsv_path}...")
        df = pd.read_table(combined_tsv_path, low_memory=False)
        print(f"Calculating columns")
        df.loc[:, "Num Repeats: Allele 1"] = df["Num Repeats: Allele 1"].fillna(0).astype(int)
        df.loc[:, "Num Repeats: Allele 2"] = df["Num Repeats: Allele 2"].fillna(0).astype(int)

        print(f"Calculating additional columns")
        df.loc[:, "Num Repeats: Min Allele 1, 2"] = df.apply(lambda row: (
            min(row["Num Repeats: Allele 1"], row["Num Repeats: Allele 2"])
            if
            row["Num Repeats: Allele 1"] > 0 and row["Num Repeats: Allele 2"] > 0
            else
            max(row["Num Repeats: Allele 1"], row["Num Repeats: Allele 2"])
        ), axis=1)

        print(f"Calculating additional columns 2")
        df.loc[:, "Num Repeats: Max Allele 1, 2"] = df[["Num Repeats: Allele 1", "Num Repeats: Allele 2"]].max(axis=1)

        # check that all required columns are present
        missing_required_columns = set(REQUIRED_COLUMNS) - set(df.columns)
        if missing_required_columns:
            raise ValueError(f"{combined_tsv_path} is missing these required columns: {missing_required_columns}")

        # fill in values for missing optional columns
        missing_optional_columns = set(OPTIONAL_COLUMNS) - set(df.columns)
        if missing_optional_columns:
            print(f"WARNING: {combined_tsv_path} is missing these columns: {missing_optional_columns}. "
                  f"Filling them with None...")
            for c in missing_optional_columns:
                df.loc[:, c] = None

        for integer_column in ("CI end: Allele 1", "CI end: Allele 2"):
            df.loc[:, integer_column] = pd.to_numeric(df[integer_column], errors="coerce")

        all_dfs.append((df, combined_tsv_path))

    return all_dfs


def print_results_for_locus(args, locus_id, locus_df, highlight_locus=False):
    """Prints the sorted table of results for a single locus"""

    for column_name in (
            "Sample_affected", "Sample_sex", "Sample_analysis_status", "Sample_coded_phenotype",
            "Sample_genome_version",
    ):
        # split values like a; b; b; and collapse to a; b
        locus_df.loc[:, column_name] = locus_df[column_name].apply(
            lambda s: "; ".join(set(s.split("; "))) if not pd.isna(s) else s)
        # truncate long text so it fits on screen
        locus_df.loc[:, column_name] = locus_df[column_name].str[0:50]

    unexpected_affected_status_values = set(locus_df[
        ~locus_df["Sample_affected"].isin({"Affected", "Not Affected", "Unknown"})
    ].Sample_affected)
    if unexpected_affected_status_values:
        print(f"WARNING: Some rows have unexpected affected value(s): {unexpected_affected_status_values}")

    # Sort
    locus_df = locus_df.sort_values(
        by=["Num Repeats: Allele 2", "Num Repeats: Allele 1", "Sample_affected"],
        ascending=False)

    # Get the 1st row and use it to look up metadata values which are the same across all rows for the locus
    # (ie. Inheritance Mode)
    first_row = locus_df.iloc[0].to_dict()

    reference_region = first_row["ReferenceRegion"]
    genome_version = f"GRCh{first_row['Sample_genome_version']}" if first_row.get('Sample_genome_version') else ""
    motif = first_row.get("RepeatUnit")
    locus_description = f"{locus_id} ({reference_region}: {genome_version})  https://stripy.org/database/{locus_id}"
    if "X" in reference_region:
        inheritance_mode = "XR"
    else:
        if compute_canonical_motif(motif) == "AAG":
            inheritance_mode = "AR"
        else:
            inheritance_mode = "AD"

    if highlight_locus:
        locus_description += " " * 100 + "  <== highlight"
    print("**Locus**: ", locus_description)
    print("**Inheritance**: ", inheritance_mode)

    # replace NA with "Unknown" strings
    locus_df.loc[:, "Sample_affected"] = locus_df["Sample_affected"].fillna("Unknown")
    locus_df.loc[:, "Sample_sex"] = locus_df["Sample_sex"].fillna("Unknown")

    # create a list of dfs that are subsets of locus_df and where rows pass thresholds
    dfs_to_process = []
    if inheritance_mode == "XR":
        # split results by male/female
        male_df = locus_df[~locus_df["Genotype"].str.contains("/")]
        dfs_to_process.append(male_df)

        female_df = locus_df[locus_df["Genotype"].str.contains("/")]
        dfs_to_process.append(female_df)

    else:
        dfs_to_process.append(locus_df)

    # filter by affected status
    filtered_dfs_list = []
    for df_to_process in dfs_to_process:
        unaffected_counter = 0
        idx = 0
        for affected_status in df_to_process["Sample_affected"]:
            idx += 1
            if affected_status and ("Not Affected" in affected_status or "Unknown" in affected_status):
                unaffected_counter += 1

            if unaffected_counter >= args.max_n_unaffected:
                break

        df_to_process = df_to_process.iloc[:idx]

        filtered_dfs_list.append(df_to_process)
    dfs_to_process = filtered_dfs_list

    for i, df_to_process in enumerate(dfs_to_process):
        print(f"Found {len(df_to_process)} samples passed filters in table {i+1} out of "
              f"{len(dfs_to_process)} for locus {locus_id}")

        if len(df_to_process) == 0:
            continue

        if inheritance_mode in {"AR", "XR"}:
            df_to_process = df_to_process.sort_values(
                by=["Num Repeats: Min Allele 1, 2", "Sample_affected"],
                ascending=False)
        elif inheritance_mode in {"AD", "XD", "Unknown"}:
            df_to_process = df_to_process.sort_values(
                by=["Num Repeats: Max Allele 1, 2", "Sample_affected"],
                ascending=False)
        else:
            raise ValueError(f"Unexpected inheritance mode: {inheritance_mode}")

        # Print the filtered results for this locus
        df_to_process = df_to_process[[
            "SampleId",
            "LocusId",

            "Sample_affected",
            "Sample_sex",
            "Sample_genome_version",

            "Genotype",
            "GenotypeConfidenceInterval",

            "VariantCatalog_Inheritance",
            "RepeatUnit",

            "Sample_analysis_status",
            "Sample_coded_phenotype",
            "Sample_phenotypes",
        ]]

        # Shorten some column names so more can fit on screen
        df_to_process.rename(columns={
            "Sample_affected": "affected",
            "Sample_sex": "sex",
            "Sample_analysis_status": "analysis_status",
            "Sample_coded_phenotype": "coded_phenotype",
            "Sample_phenotypes": "hpo",
            "Sample_genome_version": "genome",
            "GenotypeConfidenceInterval": "GenotypeCI",
            "VariantCatalog_Inheritance": "Mode",
            "RepeatUnit": "Motif",
        }, inplace=True)

        # Print the candidate pathogenic rows for this locus
        print(tabulate(df_to_process, headers="keys", tablefmt="psql", showindex=False))

    return pd.concat(dfs_to_process)


def main():
    args = parse_args()

    all_dfs = load_results_tables(args)

    all_sample_ids = set()
    all_locus_ids = set()
    for df, _ in all_dfs:
        all_locus_ids |= set(df.LocusId)
        all_sample_ids |= set(df.SampleId)

    all_variant_ids = set()
    for df, _ in all_dfs:
        all_variant_ids |= set(df.VariantId)

    variant_ids_that_are_not_locus_ids = all_variant_ids - all_locus_ids
    if variant_ids_that_are_not_locus_ids:
        print("WARNING: discarding records with VariantIds:", ", ".join(variant_ids_that_are_not_locus_ids))
        for i, (df, df_source_path) in enumerate(all_dfs):
            df = df[~df.VariantId.isin(variant_ids_that_are_not_locus_ids)]
            all_dfs[i] = (df, df_source_path)


    previously_diagnosed_loci = set()
    if args.previously_diagnosed_loci:
        for l in args.previously_diagnosed_loci:
            if not os.path.isfile(l):
                previously_diagnosed_loci.add(l)
            else:
                with open(l, "rt") as f:
                    for i, s in enumerate(f.readlines()):
                        previously_diagnosed_loci.add(s.strip())

                print(f"Read {i + 1} sample ids to highlight from: {l}")

        if previously_diagnosed_loci - all_locus_ids:
            print(f"WARNING: cannot highlight {len(previously_diagnosed_loci - all_locus_ids)} loci as they aren't in the "
                  f"main table(s): {previously_diagnosed_loci - all_locus_ids}")

        previously_diagnosed_loci = previously_diagnosed_loci & all_locus_ids
        print(f"Will highlight {len(previously_diagnosed_loci)} locus ids: {previously_diagnosed_loci}")

    highlight_sample_ids = set()
    if args.highlight_samples:
        for h in args.highlight_samples:
            if not os.path.isfile(h):
                highlight_sample_ids.add(h)
            else:
                with open(h, "rt") as f:
                    for i, s in enumerate(f.readlines()):
                        highlight_sample_ids.add(s.strip())

                print(f"Read {i + 1} sample ids to highlight from: {h}")

        if highlight_sample_ids - all_sample_ids:
            print(f"WARNING: cannot highlight {len(highlight_sample_ids - all_sample_ids)} ids as they aren't in the "
                  f"main table(s): {highlight_sample_ids - all_sample_ids}")

        highlight_sample_ids = highlight_sample_ids & all_sample_ids
        print(f"Will highlight {len(highlight_sample_ids)} sample ids: {highlight_sample_ids}")

    if args.truth_samples:
        truth_df = pd.read_table(args.truth_samples, names=["sample_id", "locus_id"] + [f"c{i}" for i in range(3, 10)])

        if set(truth_df.sample_id) - all_sample_ids:
            print(f"WARNING: cannot mark {len(set(truth_df.sample_id) - all_sample_ids)} samples ids from the truth "
                  f"samples table since they aren't in the main table(s): {set(truth_df.sample_id) - all_sample_ids}")
        if set(truth_df.locus_id) - all_locus_ids:
            print(f"WARNING: cannot mark {len(set(truth_df.locus_id) - all_locus_ids)} loci from the truth samples "
                  f"table since they aren't in the main table(s): {set(truth_df.locus_id) - all_locus_ids}")

    if args.previously_seen_samples:
        previously_seen_samples_df = pd.read_table(args.previously_seen_samples,
                                                   names=["sample_id", "locus_id"] + [f"c{i}" for i in range(3, 10)])

        if set(previously_seen_samples_df.sample_id) - all_sample_ids:
            print(f"WARNING: cannot mark {len(set(previously_seen_samples_df.sample_id) - all_sample_ids)} samples ids "
                  f"from the previously seen samples table since they aren't in the main table(s): "
                  f"{set(previously_seen_samples_df.sample_id) - all_sample_ids}")
        if set(previously_seen_samples_df.locus_id) - all_locus_ids:
            print(f"WARNING: cannot mark {len(set(previously_seen_samples_df.locus_id) - all_locus_ids)} loci from the "
                  f"previously seen sample table since they aren't in the main table(s): "
                  f"{set(previously_seen_samples_df.locus_id) - all_locus_ids}")

    # Process each locus
    results_dfs = []
    for locus_id in sorted(all_locus_ids):
        if args.locus and locus_id not in args.locus:
            continue

        # Filter to rows for the current locus
        for i, (full_df, df_source_path) in enumerate(all_dfs):
            print("="*100)  # print a divider
            print(f"** {locus_id} from {df_source_path} **")
            locus_df = full_df[full_df.LocusId == locus_id]
            if len(locus_df) == 0:
                return

            if highlight_sample_ids:
                locus_df.loc[:, "SampleId"] = locus_df["SampleId"].apply(
                    lambda s: (s if s not in highlight_sample_ids else f"==> {s}"))

            truth_samples_for_this_locus = set()
            if args.truth_samples and locus_id in set(truth_df.locus_id):
                truth_samples_for_this_locus = set(
                    truth_df[truth_df.locus_id == locus_id].sample_id)

            previously_seen_samples_for_this_locus = set()
            if args.previously_seen_samples and locus_id in set(previously_seen_samples_df.locus_id):
                previously_seen_samples_for_this_locus = set(
                    previously_seen_samples_df[previously_seen_samples_df.locus_id == locus_id].sample_id)

            locus_df.loc[:, "SampleId"] = locus_df["SampleId"].apply(
                lambda s: (
                    f"*T* {s}" if s in truth_samples_for_this_locus else (
                    f"*P* {s}" if s in previously_seen_samples_for_this_locus else s)
                )
            )

            results_df = print_results_for_locus(
                args, locus_id, locus_df, highlight_locus=locus_id in previously_diagnosed_loci)

            results_dfs.append(results_df)

    final_results_df = pd.concat(results_dfs)
    final_results_df = final_results_df[final_results_df["Sample_affected"] != SEPARATOR_STRING]
    #final_results_df.to_csv(args.results_path, index=False, header=True, sep="\t")
    #print(f"Wrote {len(final_results_df)} rows to {args.results_path}")
    for locus_id, count in sorted(dict(final_results_df.groupby("LocusId").count().SampleId).items()):
        print(f"{count:10,d}  {locus_id} rows")


if __name__ == "__main__":
    main()
