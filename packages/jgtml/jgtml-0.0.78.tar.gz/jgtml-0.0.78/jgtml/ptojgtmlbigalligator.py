# This file is generated by pto-pre-build.sh. Do not edit it manually.
# Tue 25 Jun 2024 01:35:39 PM EDT
# SOURCE NAME: jgtml_obsds_240515_TIDE_SIGNALS.py
########################
 

#%% Imports 


import pandas as pd
import sys
import subprocess

_DEVFLAG=False

CLI_DESCRIPTION = 'Process financial market data to analyze the performance of SELL signals generated by the "Fractal Divergent Bar" indicator within the "Big Alligator" technical analysis tool.'

EPILOG="""
This Python script is designed to analyze the effectiveness of signals in the context of the "Big Alligator" technical analysis tool, specifically those generated by the "Fractal Divergent Bar" indicator. The script reads historical market data for a specified instrument and timeframe, calculates technical indicators, and identifies valid SELL and BUY signals based on predefined criteria. 

The script then evaluates the profitability of each valid signal's direction, analyzes the distribution of profit potential across different signal types, and compares their performance. The results of the analysis are outputted as a CSV file, and a markdown file summarizing the key findings is also generated.
"""
X_ABSTRACT="""
# Abstract"""+EPILOG+"""



The script leverages the `jgtpy` and `jgtml` Python packages for data analysis and machine learning. It is an early prototype made during my coding mastery and is somehow customizable, allowing for the analysis of different instruments, timeframes, and signal types using the CLI. This script is part of a broader research project exploring the use of technical indicators in trading.

Future work includes further data analysis to identify additional patterns and insights, the development of a trading strategy based on the analysis findings. This script serves as a experiment to find valuable tool for improving trading strategies and decision-making by providing insights into the effectiveness of fractal divergent signals generated by the "Fractal Divergent Bar" indicator and their relationship with the "Big Alligator" tool.

The 6 types of signals analyzed in this script are:

- all_signals
- sig_nmopen
- sig_basic_filtering
- sig_is_in_bteeth
- sig_bmopen_in_bteeth
- sig_bmopen_in_blips

The script outputs the following metrics for each signal type:

- per_trade: Average profit per trade
- nb_entry: Number of valid signals for this type (could compare to the total number if signal of the "all_signals" column to learn how many signals are validated by this type of signal)
- tsum: Total profit across all signals
- title: Signal type

The script also generates a markdown file summarizing the key findings and a CSV file containing the detailed results of the analysis.

Details of each type of signals

- all_signals: All valid signals made by the jgtml.jtc.pto_target_calculation function
- sig_nmopen: Signals where the mouth of the "Regular Alligator" is open
- sig_basic_filtering: Signals where the price bar is out of the lips and teeth of the "Regular Alligator"
- sig_is_in_bteeth: Signals where the price bar has came back in the big teeth of the "Big Alligator" without the mouth being open or not.
- sig_bmopen_in_bteeth: Signals where the price bar has came back in the big teeth and the mouth of the "Big Alligator" is open (Exploring a Strategic entry when the mouth is open and we want to enter at the end of a retracement.  This explores signals that pulled back in the big teeth(the middest of the big balancing lines system))
- sig_bmopen_in_blips: Signals where the price bar has came back in the big lips and the mouth of the "Big Alligator" is open (Exploring a Strategic entry when the mouth is open and we want to enter at the end of a retracement. This explores signals that pulled back in the big lips (the smallest of the big balancing lines system) )

Further analysis and interpretation of the results are needed to draw meaningful conclusions and insights from the data. The script is intended to be a starting point for more in-depth research and exploration of technical indicators in trading.

"""

import os

import numpy as np
import pandas as pd
try:
    from jgtpy import JGTCDS as cds
except:
    print("pip install -U jgtpy")

import sys

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

try:
    from jgtml import jtc
except:
    import jtc

# Use jgtconstants column names from jgtutils
from jgtutils.jgtconstants import LOW,HIGH,FDBB,FDBS,BJAW,BLIPS,BTEETH,JAW,TEETH,LIPS,FDB_TARGET,VECTOR_AO_FDBS_COUNT,VECTOR_AO_FDBB_COUNT,VECTOR_AO_FDB_COUNT,BJAW,BLIPS,BTEETH
#FDB_TARGET=FDB_TARGET

def crop_dataframe(df, crop_last_dt: str = None, crop_start_dt: str = None):
    if crop_last_dt is not None:
        df = df[df.index <= crop_last_dt]
    if crop_start_dt is not None:
        df = df[df.index >= crop_start_dt]
    return df

#%% Contexts 
# Contexts

i = 'GBP/USD'
i = 'SPX500'
t = "H4"
t = "D1"

use_env=os.getenv("JGTENV","0")

i=os.getenv("i",i)
t=os.getenv("t",t)

# FLAGS for the PROCESS we might want to configure or read from ENV
force_regenerate_mxfiles=False if os.getenv("force_regenerate_mxfiles") == "False" or os.getenv("force_regenerate_mxfiles") == "false" or os.getenv("force_regenerate_mxfiles") == "0" else True
mfi_flag=True
balligator_flag=True
talligator_flag=True
regenerate_cds=False if os.getenv("regenerate_cds") == "False" or os.getenv("regenerate_cds") == "false" or os.getenv("regenerate_cds") == "0" else True
use_fresh=False if os.getenv("use_fresh") == "False" or os.getenv("use_fresh") == "false" or os.getenv("use_fresh") == "0" else True
# use_fresh=True if os.getenv("use_fresh") == "True" or os.getenv("use_fresh") == "true" or os.getenv("use_fresh") == "1" else False
quiet=False if os.getenv("quiet") == "False" or os.getenv("quiet") == "false" or os.getenv("quiet") == "0" else True

# OUTPUTS Files
output_path_default="/b/Dropbox/jgt" #$jgtdroot
jgtdroot=os.getenv("jgtdroot",output_path_default)
#result_drop_base_default="/b/Dropbox/jgt/drop" #$jgtdroot
output_subdir = "drop"
result_file_basename_default = "jgtml_obsds_240515_SIGNALS.result"
result_file_basename_default = os.getenv("result_file_basename_default") if os.getenv("result_file_basename_default") is not None else result_file_basename_default
result_file_basename=result_file_basename_default

# Columns to select
sel_columns_base=[HIGH,LOW,JAW,TEETH,LIPS]

sel_columns_big_alligator=[BJAW,BTEETH,BLIPS]
sel_columns_big_alligator=[BJAW,BTEETH,BLIPS]

# Select columns according to Flags (big alligator and big alligator)
sel_columns_common=sel_columns_base + \
    (sel_columns_big_alligator if balligator_flag else None) +  \
        (sel_columns_big_alligator if talligator_flag else None) + \
            [FDB_TARGET,VECTOR_AO_FDB_COUNT]

sel_columns_sell = sel_columns_common + [FDBS, VECTOR_AO_FDBS_COUNT]
sel_columns_buy = sel_columns_common + [FDBB, VECTOR_AO_FDBB_COUNT]

jgtpy_data_full_var_name = "JGTPY_DATA_FULL"

bs="S" # This prototype is for SELL signals only, we can extend it to BUY signals later
bs=os.getenv("bs",bs)

result_drop_base_override=None
source_dataset_archival_path_override=None
data_dir_override=None
archive_used_dataset=True
use_ttf_default = True




NB_CONTEXT_RUN=False
def _chg_logics_for_notebook_context():
    global use_fresh, regenerate_cds
    #TODO Chg logics for notebook context
    #regenerate_cds=False
    #use_fresh=False
    pass
    
NB_CONTEXT_RUN=False

#print(f"i:{i} t:{t} bs:{bs}")
try:
    import IPython

    if IPython.get_ipython() is not None:
        # Code is running in a Jupyter notebook
        print("Notebook context detected")
        NB_CONTEXT_RUN=True
    else:
        # Code is running outside a Jupyter notebook
        print("Notebook context not detected")
        NB_CONTEXT_RUN=False


except:
    print("Notebook context not detected")
    NB_CONTEXT_RUN=False



def print_context():
    #print("---BEFORE MAIN (Ignore when not using env (JGTENV=1))")
    print(f"i:{i} t:{t} bs:{bs}")
    print(f"regenerate_cds:{regenerate_cds}")
    print(f"Fresh:{use_fresh}")
    #print("----------------------------------------------")
    


def main():
    #global i,t,bs,print_abstract,regenerate_cds,result_drop_base_override,source_dataset_archival_path_override,quiet,data_dir_override,use_fresh,use_env
    import argparse
    parser = argparse.ArgumentParser(description=CLI_DESCRIPTION,epilog=EPILOG)
    if not use_env or use_env == "0":
        parser.add_argument('-i','--instrument', type=str, help='Instrument')
        parser.add_argument('-t','--timeframe', type=str, help='Timeframe')
        #bs
        parser.add_argument('-bs','--buysell', type=str, help='Buy or Sell')
        #regenerate_cds
        parser.add_argument('-nocds','--dont_regenerate_cds', help='Dont Regenerate CDS',action='store_true')
        #use_fresh
        parser.add_argument('-nf','--nofresh', help='Dont Use Fresh (Assuming you need CDS Already generated and available)',action='store_true')
    #parser.add_argument('-old','--not_fresh', help='Dont Use Fresh (Assuming you need CDS Already generated and available)',action='store_true')
        parser.add_argument('-q','--quiet', help='Quiet',action='store_true')

    #data_dir_override
    parser.add_argument('-dd','--data_dir_override', type=str, default=None, help='Data Directory Override path (JGTPY_DATA_FULL env var will be used if not provided)')
    #result_drop_base_override
    parser.add_argument('-rdb','--result_drop_base_override', type=str, default=None, help='Result Drop Base Override')
    #source_dataset_archival_path_override
    parser.add_argument('-sda','--source_dataset_archival_path_override', type=str, default=None, help='Source Dataset Archival Path Override')
    # quiet
    #print X_ABSTRACT
    parser.add_argument('-abstract','--print_abstract', action='store_true', help='Print Abstract')
    parser.add_argument('-ttf','--use_ttf_rather_than_cds', action='store_true', help='Force usage of TTF as source of data (CDS + HTF data)')
    #result_file_basename
    parser.add_argument('-rfb','--result_file_basename', type=str, default=result_file_basename_default, help='Result File Base Name')    
    
    parser.add_argument('-env','--use_env', help='Use environment variable instead of arguments',action='store_true')
    
    args = parser.parse_args()
    print_abstract = args.print_abstract
    
    if print_abstract:
        print(X_ABSTRACT,flush=True)
        exit(0)
        
    if not args.use_env or not use_env or use_env == "0":
        i = args.instrument
        t = args.timeframe
        bs = args.buysell
        regenerate_cds = not args.dont_regenerate_cds #default is to regenerate
        quiet = args.quiet
        use_fresh= False if args.nofresh else True
    else:
        i=os.getenv("i")
        t=os.getenv("t")
        bs=os.getenv("bs")
        dont_regenerate_cds_value=os.getenv("dont_regenerate_cds") #default is to regenerate
        regenerate_cds = True if dont_regenerate_cds_value is None or dont_regenerate_cds_value =="0" or dont_regenerate_cds_value == "False" else False

    result_drop_base_override = args.result_drop_base_override
    source_dataset_archival_path_override = args.source_dataset_archival_path_override
    data_dir_override = args.data_dir_override
        
    
    #@STCIssue We would do that by default
    use_ttf_rather_than_cds=args.use_ttf_rather_than_cds if args.use_ttf_rather_than_cds else False #@STCGoal Integration of all Signal Evaluation in one Script - We want HTF(ttf) data to be used in the evaluation so we can have a better understanding of the context of the signal with higher timeframes (Zone,MFI, etc)
    
    #return variables from args
    return i,t,bs,regenerate_cds,quiet,use_fresh,result_drop_base_override,source_dataset_archival_path_override,data_dir_override,use_ttf_rather_than_cds

if not NB_CONTEXT_RUN:
    if __name__ == "__main__":
        i,t,bs,regenerate_cds,quiet,use_fresh,result_drop_base_override,source_dataset_archival_path_override,data_dir_override,use_ttf_rather_than_cds=main()
else:
    _chg_logics_for_notebook_context()

print_context()
print("--------AFTER MAIN---------")
print(f"i:{i} t:{t} bs:{bs}")


# Where to save the results
def create_result_directory(jgtdroot, drop_subdir, result_drop_base_override):
    result_drop_base=os.path.join(jgtdroot, drop_subdir) if result_drop_base_override is None else result_drop_base_override

    if not os.path.exists(result_drop_base):
        result_drop_base=os.path.join(".", drop_subdir)
        os.makedirs(result_drop_base,exist_ok=True)
    return result_drop_base

result_drop_base = create_result_directory(jgtdroot, output_subdir, result_drop_base_override)
    

#Where source dataset is archived 

def get_arch_tdir():
    return "jgtml_240516"

def get_source_dataset_archival_path(source_dataset_archival_path_override, result_drop_base, get_arch_tdir):
    source_dataset_archival_path= os.path.join(result_drop_base, "data", "arch", get_arch_tdir()) if source_dataset_archival_path_override is None else source_dataset_archival_path_override
    return source_dataset_archival_path

source_dataset_archival_path = get_source_dataset_archival_path(source_dataset_archival_path_override, result_drop_base, get_arch_tdir)


# Columns to select based on the direction    
if bs=="S" or bs=="SELL" or bs=="sell":
    sel_columns = sel_columns_sell
else:
    if bs=="B" or bs=="BUY" or bs=="buy":
        sel_columns = sel_columns_buy

result_file_base = os.path.join(result_drop_base, result_file_basename)

result_file_md=result_file_base + ".md"
result_file_csv=result_file_base + ".csv"


try:
    os.makedirs(source_dataset_archival_path,exist_ok=True)
except:
    #exit the app with error about the directory
    print(f"Error creating directory {source_dataset_archival_path}")
    exit(1)



ifn=i.replace("/","-")
data_dir = os.getenv(jgtpy_data_full_var_name) if data_dir_override is None else data_dir_override
#idsfilepath = os.path.join(data_dir, "targets", "mx", f"{ifn}_{t}.csv")
#df = pd.read_csv(idsfilepath)
df=None
try:
    if not force_regenerate_mxfiles:
        df = jtc.readMXFile(i,t)
except:
    pass

#%pip install -U jgtpy

#set DF to None if column 'mfi' is not present (force regeneration)
if df is not None and 'mfi' not in df.columns: #TMP to force regeneration if column mfi is not present
    df = None

if df is None:
    df, sel1, sel2 = jtc.pto_target_calculation(i,t,
                                                mfi_flag=mfi_flag,talligator_flag=talligator_flag,
                                                balligator_flag=balligator_flag,
                                                regenerate_cds=regenerate_cds,
                                                use_fresh=use_fresh,
                                                use_ttf=use_ttf_default)

#%%
if NB_CONTEXT_RUN:
    quiet=False if os.getenv("quiet") is None else quiet

if not quiet:
    l=len(df)
    print(df.tail(1))
    print(df.columns)
    print("len(df):",len(df))

#%% 
"""
'Volume', 'Open', HIGH, LOW, 'Close', 'Median', 'ao', 'ac', JAW,
       TEETH, LIPS, 'fh', 'fl', 'fh3', 'fl3', 'fh5', 'fl5', 'fh8', 'fl8',
       'fh13', 'fl13', 'fh21', 'fl21', 'fh34', 'fl34', 'fh55', 'fl55', 'fh89',
       'fl89', 'fdbb', 'fdbs', 'fdb', 'aoaz', 'aobz', 'zlc', 'zlcb', 'zlcs',
       'zcol', 'sz', 'bz', 'acs', 'acb', 'ss', 'sb', 'price_peak_above',
       'price_peak_bellow', 'ao_peak_above', 'ao_peak_bellow', 'tmax', 'tmin',
       'p', 'l', TARGET, 'vaos', 'vaob', 'vaosc', 'vaobc', 'vaoc'],
      
"""

# create a dataset with only the columns we need.  
dfo = df[sel_columns]


# %%
if not quiet:
    print(dfo.columns)

# %%
# Select row where target is not 0
dfo = dfo[dfo[FDB_TARGET] != 0] 
#dfoprofit = dfo[dfo[TARGET] > 0] 
# %%
if not quiet:
    print(dfo)


#%% Direction
direction = bs #Might use only one variable in the future to avoid confusion
# %%
dfo_context = dfo[sel_columns].copy()

#
fdb_context_colname = FDBS
if direction=="B":
    fdb_context_colname = FDBB
dfo_context = dfo_context[dfo_context[fdb_context_colname] != 0].copy()





# dfo = dfo[dfo[TARGET] != 0] #Where target is not 0
# %%
dfo_context.tail(40)
# %%
all_context_signal_count = len(dfo_context)

target_colname = FDB_TARGET
all_signalsnal_sum=dfo_context[target_colname].sum()

if _DEVFLAG:
    print("count:",all_context_signal_count," sum0:",all_signalsnal_sum)

# %%
#Remove invalid signal when column High < lips
#@STCGoal Valid Signals are bellow the lips and teeth 
if bs=="S":
    sig_basic_filtering = dfo_context[dfo_context[LOW] > dfo_context[LIPS]].copy()
    sig_basic_filtering = sig_basic_filtering[sig_basic_filtering[LOW] > sig_basic_filtering[TEETH]]

    #@STCGoal Valid Signals when mouth is open
    sig_nmopen = sig_basic_filtering[sig_basic_filtering[JAW] < sig_basic_filtering[TEETH]].copy()
    sig_nmopen = sig_nmopen[sig_nmopen[TEETH] < sig_nmopen[LIPS]]
    sig_nmopen = sig_nmopen[sig_nmopen[JAW] < sig_nmopen[LIPS]]

    #sig_is_in_bteeth
    sig_is_in_bteeth = sig_basic_filtering[
    sig_basic_filtering[LOW] > sig_basic_filtering[BTEETH]
    ].copy()

    #sig_bmopen_in_bteeth
    sig_bmopen_in_bteeth = sig_basic_filtering[
        sig_basic_filtering[LOW] > sig_basic_filtering[BTEETH]
        ].copy()

    sig_bmopen_in_bteeth = sig_bmopen_in_bteeth[  #@a The Big Mouth Is Open
        sig_bmopen_in_bteeth[BLIPS] < sig_bmopen_in_bteeth[BTEETH]
        ]
    sig_bmopen_in_bteeth = sig_bmopen_in_bteeth[  #@a The Big Mouth Is Open
        sig_bmopen_in_bteeth[BTEETH] < sig_bmopen_in_bteeth[BJAW]
        ]        

    #sig_bmopen_in_blips
    sig_bmopen_in_blips = sig_basic_filtering[
        sig_basic_filtering[LOW] < sig_basic_filtering[BLIPS]
        ].copy()

    # the BMouth is Openned
    sig_bmopen_in_blips = sig_bmopen_in_blips[
        sig_bmopen_in_blips[BLIPS] < sig_bmopen_in_blips[BTEETH]
        ]
    sig_bmopen_in_blips = sig_bmopen_in_blips[
        sig_bmopen_in_blips[BTEETH] < sig_bmopen_in_blips[BJAW]
        ]
    
    #
                    

else:
    sig_basic_filtering = dfo_context[dfo_context[HIGH] < dfo_context[LIPS]].copy()
    sig_basic_filtering = sig_basic_filtering[sig_basic_filtering[HIGH] < sig_basic_filtering[TEETH]]

    #sig_nmopen
    sig_nmopen = sig_basic_filtering[sig_basic_filtering[JAW] > sig_basic_filtering[TEETH]].copy()
    sig_nmopen = sig_nmopen[sig_nmopen[TEETH] > sig_nmopen[LIPS]]
    sig_nmopen = sig_nmopen[sig_nmopen[JAW] > sig_nmopen[LIPS]]

    #sig_is_in_bteeth
    sig_is_in_bteeth = sig_basic_filtering[
    sig_basic_filtering[HIGH] < sig_basic_filtering[BTEETH]
    ].copy()

    #
    #sig_bmopen_in_bteeth
    sig_bmopen_in_bteeth = sig_basic_filtering[
        sig_basic_filtering[HIGH] < sig_basic_filtering[BTEETH]
        ].copy()

    sig_bmopen_in_bteeth = sig_bmopen_in_bteeth[  #@a The Big Mouth Is Open
        sig_bmopen_in_bteeth[BLIPS] > sig_bmopen_in_bteeth[BTEETH]
        ]
    sig_bmopen_in_bteeth = sig_bmopen_in_bteeth[  #@a The Big Mouth Is Open
        sig_bmopen_in_bteeth[BTEETH] > sig_bmopen_in_bteeth[BJAW]
        ]      
    
    #
    #sig_bmopen_in_blips
    sig_bmopen_in_blips = sig_basic_filtering[
        sig_basic_filtering[HIGH] > sig_basic_filtering[BLIPS]
        ].copy()

    # the BMouth is Openned
    sig_bmopen_in_blips = sig_bmopen_in_blips[
        sig_bmopen_in_blips[BLIPS] > sig_bmopen_in_blips[BTEETH]
        ]
    sig_bmopen_in_blips = sig_bmopen_in_blips[
        sig_bmopen_in_blips[BTEETH] > sig_bmopen_in_blips[BJAW]
        ]
    
    #






# INDEPENDENT OF DIRECTIONS
sig_basic_filtering_count = len(sig_basic_filtering)
sig_basic_filtering_sum=sig_basic_filtering[FDB_TARGET].sum()

sig_nmopen_count = len(sig_nmopen)
sig_nmopen_sum=sig_nmopen[FDB_TARGET].sum()





if not quiet:
    print("sig_basic_filtering:",sig_basic_filtering_count," sum:",sig_basic_filtering_sum)


# %%

if not quiet:
    print(sig_basic_filtering.tail(40))
    print("count3:",sig_nmopen_count," sum3:",sig_nmopen_sum)

# %%

if not quiet:
    print(sig_nmopen.tail(40))


# %%
if not quiet:
    print("count (no validation):",all_context_signal_count," sum0:",all_signalsnal_sum)
    print("count_2 sig_basic_filtering:",sig_basic_filtering_count," sum2:",sig_basic_filtering_sum)
    print("count_sell3 sig_nmopen:",sig_nmopen_count," sum3:",sig_nmopen_sum)






# # %%
# dfosell_vaoc_min_bar.tail(20)
# %%

# %%

result_of_this_analysis = """
Best Validation : not in lips and teeth

NEXT STEP : 

* Larger Timeframe insights.
"""


#@STCGoal  Eval Big Mouth 
# %%  BIG ALLIGATOR SELL - Eval Low is Bellow Big Teeth



sig_is_in_bteeth_count = len(sig_is_in_bteeth)
sig_is_in_bteeth_sum=sig_is_in_bteeth[FDB_TARGET].sum()

# %%

if not quiet:
    print(sig_is_in_bteeth.tail(40))
    print("sig_is_in_bteeth:",sig_is_in_bteeth_count," sum:",sig_is_in_bteeth_sum)




# %%  BIG ALLIGATOR  - Eval Low is Above Big Lips and Big Mouth is Open OR RELATED tO BUY

                                       
sig_bmopen_in_bteeth_count = len(sig_bmopen_in_bteeth)
sig_bmopen_in_bteeth_sum=sig_bmopen_in_bteeth[FDB_TARGET].sum()

# %%

if not quiet:
    print(sig_bmopen_in_bteeth.tail(40))
    print("count_sell2s2 Low is Above Big Teeth and Big Mouth is Open:",sig_bmopen_in_bteeth_count," sum2s2:",sig_bmopen_in_bteeth_sum)


# %%  BIG ALLIGATOR - Eval Low is Above Big Lips and Big Mouth is Open OR RELATED tO BUY

                                                                    
sig_bmopen_in_blips_count = len(sig_bmopen_in_blips)
sig_bmopen_in_blips_sum=sig_bmopen_in_blips[FDB_TARGET].sum()

# %%
if not quiet:
    print(sig_bmopen_in_blips.tail(40))
    print("count_sell2s2 Low is Above Big Lips and Big Mouth is Open:",sig_bmopen_in_blips_count," sum2s2:",sig_bmopen_in_blips_sum)





# %%


def write_to_result_csv(i,t,direction,nb_entry, tsum, title,_df=None):
    per_trade=round(tsum/nb_entry,2)
    if direction == "sell" or direction=="Sell" or direction=="SELL":direction="S"
    if direction == "buy" or direction=="Buy" or direction=="BUY":direction="B"
    
    #per_trade="per_trade"
    with open(result_file_csv, "a") as file_object:
        tsum_rounded = round(tsum,2)
        file_object.write(f"{i},{t},{direction},{per_trade},{nb_entry},{tsum_rounded},{title}\n")
    if _df is not None:
        save_df_archives(i, t, title, _df)

def save_df_archives(i, t, title, _df,quiet=True):
    ifn = i.replace("/","-")
    csv_fn = f"{source_dataset_archival_path}/{ifn}_{t}_{title}.csv"
    if not quiet:
        print("Writing CSV to file:",csv_fn)
    _df.to_csv(csv_fn)

#save the original df
if archive_used_dataset:
    save_df_archives(i, t, "original", df)
#write_to_result_csv("instrument","timeframe","nb_entry","tsum","title")

def write_to_result_md(entry_line):
    with open(result_file_md, "a") as file_object:
        file_object.write(f"{entry_line}\n")

def print_res(i,t,direction,nb_entry, tsum, title,_df=None):
    per_trade=round(tsum/nb_entry,2)
    tsum_rounded = round(tsum,2)
    entry_line = f"{i}_{t} {direction} pt:{per_trade} t:{nb_entry} sum:{tsum_rounded} title:{title}"
    print(entry_line)
    write_to_result_md(entry_line)
    write_to_result_csv(i,t,direction,nb_entry, tsum, title,_df)
# %%

write_to_result_md("  ")
write_to_result_md("----")
write_to_result_md("  ")
write_to_result_md("==============================================================")
print_res(i,t,direction,all_context_signal_count,all_signalsnal_sum,"all_signals",dfo_context)
print_res(i,t,direction,sig_nmopen_count,sig_nmopen_sum,"sig_nmopen",sig_nmopen)
write_to_result_md("==============================================================")
print_res(i,t,direction,sig_basic_filtering_count,sig_basic_filtering_sum,"sig_basic_filtering",sig_basic_filtering)
write_to_result_md("==============================================================")
print_res(i,t,direction,sig_is_in_bteeth_count,sig_is_in_bteeth_sum,"sig_is_in_bteeth_sum",sig_is_in_bteeth)
print_res(i,t,direction,sig_bmopen_in_blips_count,sig_bmopen_in_blips_sum,"sig_bmopen_in_blips",sig_bmopen_in_blips)
#pt:32.42 t:132 sum:4279.999999999999 title:count_sell2s2 Low is Above Big Lips and Big Mouth is Open
print_res(i,t,direction,sig_bmopen_in_bteeth_count,sig_bmopen_in_bteeth_sum,"sig_bmopen_in_bteeth_sum",sig_bmopen_in_bteeth)
write_to_result_md("==============================================================")


#@STCIssue SPX500 D1, sig_bmopen_in_bteeth_sum has an interesting value when the Signal bar is in the big mouth
#pt:83.2 t:99 sum:8237.0 title:sig_bmopen_in_bteeth_sum
# %%
print("Saved md: ",result_file_md)
print("Saved csv: ",result_file_csv)
print("Completed processing for ",i,t,direction)