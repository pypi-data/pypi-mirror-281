# This file is generated by pto-pre-build.sh. Do not edit it manually.
# Tue 25 Jun 2024 06:27:45 PM EDT
# SOURCE NAME: jgtml_obsds_240515_TIDE_SIGNALS.py
########################
 

#%% Imports 


import pandas as pd
import sys
import subprocess

_DEVFLAG=False

CLI_DESCRIPTION = 'Process financial market data to analyze the performance of SELL signals generated by the "Fractal Divergent Bar" indicator within the "Tide Alligator" technical analysis tool.'

EPILOG="""
This Python script is designed to analyze the effectiveness of signals in the context of the "Tide Alligator" technical analysis tool, specifically those generated by the "Fractal Divergent Bar" indicator. The script reads historical market data for a specified instrument and timeframe, calculates technical indicators, and identifies valid SELL and BUY signals based on predefined criteria. 

The script then evaluates the profitability of each valid signal's direction, analyzes the distribution of profit potential across different signal types, and compares their performance. The results of the analysis are outputted as a CSV file, and a markdown file summarizing the key findings is also generated.
"""
X_ABSTRACT="""
# Abstract"""+EPILOG+"""



The script leverages the `jgtpy` and `jgtml` Python packages for data analysis and machine learning. It is an early prototype made during my coding mastery and is somehow customizable, allowing for the analysis of different instruments, timeframes, and signal types using the CLI. This script is part of a broader research project exploring the use of technical indicators in trading.

Future work includes further data analysis to identify additional patterns and insights, the development of a trading strategy based on the analysis findings. This script serves as a experiment to find valuable tool for improving trading strategies and decision-making by providing insights into the effectiveness of fractal divergent signals generated by the "Fractal Divergent Bar" indicator and their relationship with the "Tide Alligator" tool.

The 6 types of signals analyzed in this script are:

- all_signals
- sig_nmopen
- sig_basic_filtering
- sig_is_in_tteeth
- sig_tmopen_in_tteeth
- sig_tmopen_in_tlips

The script outputs the following metrics for each signal type:

- per_trade: Average profit per trade
- nb_entry: Number of valid signals for this type (could compare to the total number if signal of the "all_signals" column to learn how many signals are validated by this type of signal)
- tsum: Total profit across all signals
- title: Signal type

The script also generates a markdown file summarizing the key findings and a CSV file containing the detailed results of the analysis.

Details of each type of signals

- all_signals: All valid signals made by the jgtml.jtc.pto_target_calculation function
- sig_nmopen: Signals where the mouth of the "Regular Alligator" is open
- sig_basic_filtering: Signals where the price bar is out of the lips and teeth of the "Regular Alligator"
- sig_is_in_tteeth: Signals where the price bar has came back in the big teeth of the "Big Alligator" without the mouth being open or not.
- sig_tmopen_in_tteeth: Signals where the price bar has came back in the big teeth and the mouth of the "Big Alligator" is open (Exploring a Strategic entry when the mouth is open and we want to enter at the end of a retracement.  This explores signals that pulled back in the big teeth(the middest of the big balancing lines system))
- sig_tmopen_in_tlips: Signals where the price bar has came back in the big lips and the mouth of the "Big Alligator" is open (Exploring a Strategic entry when the mouth is open and we want to enter at the end of a retracement. This explores signals that pulled back in the big lips (the smallest of the big balancing lines system) )

Further analysis and interpretation of the results are needed to draw meaningful conclusions and insights from the data. The script is intended to be a starting point for more in-depth research and exploration of technical indicators in trading.

"""

import os

import numpy as np
import pandas as pd
try:
    from jgtpy import JGTCDS as cds
except:
    print("pip install -U jgtpy")

import sys

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

try:
    from jgtml import jtc
except:
    import jtc

# Use jgtconstants column names from jgtutils
from jgtutils.jgtconstants import LOW,HIGH,FDBB,FDBS,BJAW,BLIPS,BTEETH,JAW,TEETH,LIPS,FDB_TARGET,VECTOR_AO_FDBS_COUNT,VECTOR_AO_FDBB_COUNT,VECTOR_AO_FDB_COUNT,TJAW,TLIPS,TTEETH
#FDB_TARGET=FDB_TARGET

def crop_dataframe(df, crop_last_dt: str = None, crop_start_dt: str = None):
    if crop_last_dt is not None:
        df = df[df.index <= crop_last_dt]
    if crop_start_dt is not None:
        df = df[df.index >= crop_start_dt]
    return df

#%% Contexts 
# Contexts

i = 'GBP/USD'
i = 'SPX500'
t = "H4"
t = "D1"

use_env=os.getenv("JGTENV","0")

i=os.getenv("i",i)
t=os.getenv("t",t)

# FLAGS for the PROCESS we might want to configure or read from ENV
force_regenerate_mxfiles=False if os.getenv("force_regenerate_mxfiles") == "False" or os.getenv("force_regenerate_mxfiles") == "false" or os.getenv("force_regenerate_mxfiles") == "0" else True
mfi_flag=True
balligator_flag=True
talligator_flag=True
regenerate_cds=False if os.getenv("regenerate_cds") == "False" or os.getenv("regenerate_cds") == "false" or os.getenv("regenerate_cds") == "0" else True
use_fresh=False if os.getenv("use_fresh") == "False" or os.getenv("use_fresh") == "false" or os.getenv("use_fresh") == "0" else True
# use_fresh=True if os.getenv("use_fresh") == "True" or os.getenv("use_fresh") == "true" or os.getenv("use_fresh") == "1" else False
quiet=False if os.getenv("quiet") == "False" or os.getenv("quiet") == "false" or os.getenv("quiet") == "0" else True

# OUTPUTS Files
output_path_default="/b/Dropbox/jgt" #$jgtdroot
jgtdroot=os.getenv("jgtdroot",output_path_default)
#result_drop_base_default="/b/Dropbox/jgt/drop" #$jgtdroot
output_subdir = "drop"
result_file_basename_default = "jgtml_obsds_240515_SIGNALS.result"
result_file_basename_default = os.getenv("result_file_basename_default") if os.getenv("result_file_basename_default") is not None else result_file_basename_default
result_file_basename=result_file_basename_default

# Columns to select that are part of the settings in this whole process

def getBaseColumns():
    sel_columns_base=[HIGH,LOW,JAW,TEETH,LIPS]
    return sel_columns_base


def get_tide_alligator_columns():
    sel_columns_tide_alligator=[TJAW,TTEETH,TLIPS]
    return sel_columns_tide_alligator

def get_big_alligator_columns():
    sel_columns_big_alligator=[BJAW,BTEETH,BLIPS]
    return sel_columns_big_alligator

sel_columns_base = getBaseColumns()
sel_columns_tide_alligator = get_tide_alligator_columns()
sel_columns_big_alligator = get_big_alligator_columns()

eval_target_colname = FDB_TARGET # We will want to choose different target column in further analysis, for now we are using the FDB_TARGET
eval_signal_sell_colname=FDBS
eval_signal_buy_colname=FDBB

#@STCIssue How are these columns called in the process (future extra column that we can learn from) ? 
eval_extra_vector_signals_count_colname=VECTOR_AO_FDB_COUNT
eval_extra_vector_signal_sell_count_colname=VECTOR_AO_FDBS_COUNT
eval_extra_vector_signal_buy_count_colname=VECTOR_AO_FDBB_COUNT


# Select columns according to Flags (big alligator and tide alligator)
sel_columns_common=sel_columns_base + \
    (sel_columns_big_alligator if balligator_flag else None) +  \
        (sel_columns_tide_alligator if talligator_flag else None) + \
            [eval_target_colname,eval_extra_vector_signals_count_colname]

"""
FUTURE POSSIBLE COLUMN NAMES

    sell_signal_columns
    sell_indicator_fields
    sell_analysis_features
    sell_criteria_columns
    sell_trigger_attributes

"""

sel_columns_sell = sel_columns_common + [eval_signal_sell_colname,eval_extra_vector_signal_sell_count_colname ]
sel_columns_buy = sel_columns_common + [eval_signal_buy_colname,eval_extra_vector_signal_buy_count_colname ]

jgtpy_data_full_var_name = "JGTPY_DATA_FULL"

bs="S" # This prototype is for SELL signals only, we can extend it to BUY signals later
bs=os.getenv("bs",bs)

result_drop_base_override=None
source_dataset_archival_path_override=None
data_dir_override=None
archive_used_dataset=True
use_ttf_default = True




NB_CONTEXT_RUN=False
def _chg_logics_for_notebook_context():
    global use_fresh, regenerate_cds
    #TODO Chg logics for notebook context
    #regenerate_cds=False
    #use_fresh=False
    pass
    
NB_CONTEXT_RUN=False

#print(f"i:{i} t:{t} bs:{bs}")
try:
    import IPython

    if IPython.get_ipython() is not None:
        # Code is running in a Jupyter notebook
        print("Notebook context detected")
        NB_CONTEXT_RUN=True
    else:
        # Code is running outside a Jupyter notebook
        print("Notebook context not detected")
        NB_CONTEXT_RUN=False


except:
    print("Notebook context not detected")
    NB_CONTEXT_RUN=False



def print_context():
    #print("---BEFORE MAIN (Ignore when not using env (JGTENV=1))")
    print(f"i:{i} t:{t} bs:{bs}")
    print(f"regenerate_cds:{regenerate_cds}")
    print(f"Fresh:{use_fresh}")
    #print("----------------------------------------------")
    


def main():
    #global i,t,bs,print_abstract,regenerate_cds,result_drop_base_override,source_dataset_archival_path_override,quiet,data_dir_override,use_fresh,use_env
    import argparse
    parser = argparse.ArgumentParser(description=CLI_DESCRIPTION,epilog=EPILOG)
    if not use_env or use_env == "0":
        parser.add_argument('-i','--instrument', type=str, help='Instrument')
        parser.add_argument('-t','--timeframe', type=str, help='Timeframe')
        #bs
        parser.add_argument('-bs','--buysell', type=str, help='Buy or Sell')
        #regenerate_cds
        parser.add_argument('-nocds','--dont_regenerate_cds', help='Dont Regenerate CDS',action='store_true')
        #use_fresh
        parser.add_argument('-nf','--nofresh', help='Dont Use Fresh (Assuming you need CDS Already generated and available)',action='store_true')
    #parser.add_argument('-old','--not_fresh', help='Dont Use Fresh (Assuming you need CDS Already generated and available)',action='store_true')
        parser.add_argument('-q','--quiet', help='Quiet',action='store_true')

    #data_dir_override
    parser.add_argument('-dd','--data_dir_override', type=str, default=None, help='Data Directory Override path (JGTPY_DATA_FULL env var will be used if not provided)')
    #result_drop_base_override
    parser.add_argument('-rdb','--result_drop_base_override', type=str, default=None, help='Result Drop Base Override')
    #source_dataset_archival_path_override
    parser.add_argument('-sda','--source_dataset_archival_path_override', type=str, default=None, help='Source Dataset Archival Path Override')
    # quiet
    #print X_ABSTRACT
    parser.add_argument('-abstract','--print_abstract', action='store_true', help='Print Abstract')
    parser.add_argument('-ttf','--use_ttf_rather_than_cds', action='store_true', help='Force usage of TTF as source of data (CDS + HTF data)')
    #result_file_basename
    parser.add_argument('-rfb','--result_file_basename', type=str, default=result_file_basename_default, help='Result File Base Name')    
    
    parser.add_argument('-env','--use_env', help='Use environment variable instead of arguments',action='store_true')
    
    args = parser.parse_args()
    print_abstract = args.print_abstract
    
    if print_abstract:
        print(X_ABSTRACT,flush=True)
        exit(0)
        
    if not args.use_env or not use_env or use_env == "0":
        i = args.instrument
        t = args.timeframe
        bs = args.buysell
        regenerate_cds = not args.dont_regenerate_cds #default is to regenerate
        quiet = args.quiet
        use_fresh= False if args.nofresh else True
    else:
        i=os.getenv("i")
        t=os.getenv("t")
        bs=os.getenv("bs")
        dont_regenerate_cds_value=os.getenv("dont_regenerate_cds") #default is to regenerate
        regenerate_cds = True if dont_regenerate_cds_value is None or dont_regenerate_cds_value =="0" or dont_regenerate_cds_value == "False" else False

    result_drop_base_override = args.result_drop_base_override
    source_dataset_archival_path_override = args.source_dataset_archival_path_override
    data_dir_override = args.data_dir_override
        
    
    #@STCIssue We would do that by default
    use_ttf_rather_than_cds=args.use_ttf_rather_than_cds if args.use_ttf_rather_than_cds else False #@STCGoal Integration of all Signal Evaluation in one Script - We want HTF(ttf) data to be used in the evaluation so we can have a better understanding of the context of the signal with higher timeframes (Zone,MFI, etc)
    
    #return variables from args
    return i,t,bs,regenerate_cds,quiet,use_fresh,result_drop_base_override,source_dataset_archival_path_override,data_dir_override,use_ttf_rather_than_cds

if not NB_CONTEXT_RUN:
    if __name__ == "__main__":
        i,t,bs,regenerate_cds,quiet,use_fresh,result_drop_base_override,source_dataset_archival_path_override,data_dir_override,use_ttf_rather_than_cds=main()
else:
    _chg_logics_for_notebook_context()

print_context()
print("--------AFTER MAIN---------")
print(f"i:{i} t:{t} bs:{bs}")


# Where to save the results
def create_result_directory(jgtdroot, drop_subdir, result_drop_base_override):
    result_drop_base=os.path.join(jgtdroot, drop_subdir) if result_drop_base_override is None else result_drop_base_override

    if not os.path.exists(result_drop_base):
        result_drop_base=os.path.join(".", drop_subdir)
        os.makedirs(result_drop_base,exist_ok=True)
    return result_drop_base

result_drop_base = create_result_directory(jgtdroot, output_subdir, result_drop_base_override)
    

#Where source dataset is archived 

def get_arch_tdir():
    return "jgtml_240516"

def get_source_dataset_archival_path(source_dataset_archival_path_override, result_drop_base, get_arch_tdir):
    source_dataset_archival_path= os.path.join(result_drop_base, "data", "arch", get_arch_tdir()) if source_dataset_archival_path_override is None else source_dataset_archival_path_override
    return source_dataset_archival_path

source_dataset_archival_path = get_source_dataset_archival_path(source_dataset_archival_path_override, result_drop_base, get_arch_tdir)


# Columns to select based on the direction    
def make_columns_list_from_bs(sel_columns_sell, sel_columns_buy, bs):
    if bs=="S" or bs=="SELL" or bs=="sell":
        sel_columns = sel_columns_sell
    else:
        if bs=="B" or bs=="BUY" or bs=="buy":
            sel_columns = sel_columns_buy
    return sel_columns

sel_columns = make_columns_list_from_bs(sel_columns_sell, sel_columns_buy, bs)

def prepare_result_files(result_file_basename, result_drop_base):
    result_file_base = os.path.join(result_drop_base, result_file_basename)

    result_file_md=result_file_base + ".md"
    result_file_csv=result_file_base + ".csv"
    return result_file_md,result_file_csv

result_file_md, result_file_csv = prepare_result_files(result_file_basename, result_drop_base)


def ensure_archival_directory_exists(source_dataset_archival_path):
    try:
        os.makedirs(source_dataset_archival_path,exist_ok=True)
    except:
    #exit the app with error about the directory
        print(f"Error creating directory {source_dataset_archival_path}")
        exit(1)

ensure_archival_directory_exists(source_dataset_archival_path)





def get_pto_dataframe_mx_based_en_ttf(i, t, force_regenerate_mxfiles, mfi_flag, balligator_flag, talligator_flag, regenerate_cds, use_fresh, use_ttf_default):
    df=None
    try:
        if not force_regenerate_mxfiles:
            df = jtc.readMXFile(i,t)
    except:
        pass

#%pip install -U jgtpy

#set DF to None if column 'mfi' is not present (force regeneration)
    if df is not None and 'mfi' not in df.columns: #TMP to force regeneration if column mfi is not present
        df = None

    if df is None:
        df, sel1, sel2 = jtc.pto_target_calculation(i,t,
                                                mfi_flag=mfi_flag,talligator_flag=talligator_flag,
                                                balligator_flag=balligator_flag,
                                                regenerate_cds=regenerate_cds,
                                                use_fresh=use_fresh,
                                                use_ttf=use_ttf_default)
                                                    
    return df

if _DEVFLAG:
    use_fresh=False
    regenerate_cds=False
    print("DEVFLAG: use_fresh:",use_fresh)
    
df = get_pto_dataframe_mx_based_en_ttf(i, t, force_regenerate_mxfiles, mfi_flag, balligator_flag, talligator_flag, regenerate_cds, use_fresh, use_ttf_default)

#%%
if NB_CONTEXT_RUN:
    quiet=False if os.getenv("quiet") is None else quiet

if not quiet:
    l=len(df)
    print(df.tail(1))
    print(df.columns)
    print("len(df):",len(df))

#%% 

statement_from_this_point_and_next="""
From this point, we start evaluating the performance of the signals generated by the "Fractal Divergent Bar" indicator within the "Tide Alligator" technical analysis tool. We will analyze the effectiveness of the signals in the context of the "Tide Alligator" tool and evaluate the profitability of each signal type.
      
"""



# create a dataset with only the columns we need.  
dfo = df[sel_columns]


# %%
if not quiet:
    print(dfo.columns)

# %%
# Select row where target is not 0
dfo = dfo[dfo[FDB_TARGET] != 0] 
#dfoprofit = dfo[dfo[TARGET] > 0] 
# %%
if not quiet:
    print(dfo)


#%% Direction
print("Direction:",bs)
# %%
dfo_context = dfo[sel_columns].copy()

#
fdb_context_colname = FDBS
if bs=="B":
    fdb_context_colname = FDBB
dfo_context = dfo_context[dfo_context[fdb_context_colname] != 0].copy()





# dfo = dfo[dfo[TARGET] != 0] #Where target is not 0
# %%
if _DEVFLAG:
    dfo_context.tail(40)
# %%
all_context_signal_count = len(dfo_context)


all_signalsnal_sum=dfo_context[eval_target_colname].sum()

if _DEVFLAG:
    print("count:",all_context_signal_count," sum0:",all_signalsnal_sum)

# %%
#Remove invalid signal when column High < lips
#@STCGoal Valid Signals are bellow the lips and teeth 

if bs=="S":
    sig_basic_filtering = dfo_context[dfo_context[LOW] > dfo_context[LIPS]].copy()
    sig_basic_filtering = sig_basic_filtering[sig_basic_filtering[LOW] > sig_basic_filtering[TEETH]]

    #@STCGoal Valid Signals when mouth is open
    sig_nmopen = sig_basic_filtering[sig_basic_filtering[JAW] < sig_basic_filtering[TEETH]].copy()
    sig_nmopen = sig_nmopen[sig_nmopen[TEETH] < sig_nmopen[LIPS]]
    sig_nmopen = sig_nmopen[sig_nmopen[JAW] < sig_nmopen[LIPS]]

    #sig_is_in_tteeth
    sig_is_in_tteeth = sig_basic_filtering[
    sig_basic_filtering[LOW] > sig_basic_filtering[TTEETH]
    ].copy()

    #sig_tmopen_in_tteeth
    sig_tmopen_in_tteeth = sig_basic_filtering[
        sig_basic_filtering[LOW] > sig_basic_filtering[TTEETH]
        ].copy()

    sig_tmopen_in_tteeth = sig_tmopen_in_tteeth[  #@a The Big Mouth Is Open
        sig_tmopen_in_tteeth[TLIPS] < sig_tmopen_in_tteeth[TTEETH]
        ]
    sig_tmopen_in_tteeth = sig_tmopen_in_tteeth[  #@a The Big Mouth Is Open
        sig_tmopen_in_tteeth[TTEETH] < sig_tmopen_in_tteeth[TJAW]
        ]        

    #sig_tmopen_in_tlips
    sig_tmopen_in_tlips = sig_basic_filtering[
        sig_basic_filtering[LOW] < sig_basic_filtering[TLIPS]
        ].copy()

    # the BMouth is Openned
    sig_tmopen_in_tlips = sig_tmopen_in_tlips[
        sig_tmopen_in_tlips[TLIPS] < sig_tmopen_in_tlips[TTEETH]
        ]
    sig_tmopen_in_tlips = sig_tmopen_in_tlips[
        sig_tmopen_in_tlips[TTEETH] < sig_tmopen_in_tlips[TJAW]
        ]
    
    #
                    

else:
    sig_basic_filtering = dfo_context[dfo_context[HIGH] < dfo_context[LIPS]].copy()
    sig_basic_filtering = sig_basic_filtering[sig_basic_filtering[HIGH] < sig_basic_filtering[TEETH]]

    #sig_nmopen
    sig_nmopen = sig_basic_filtering[sig_basic_filtering[JAW] > sig_basic_filtering[TEETH]].copy()
    sig_nmopen = sig_nmopen[sig_nmopen[TEETH] > sig_nmopen[LIPS]]
    sig_nmopen = sig_nmopen[sig_nmopen[JAW] > sig_nmopen[LIPS]]

    #sig_is_in_tteeth
    sig_is_in_tteeth = sig_basic_filtering[
    sig_basic_filtering[HIGH] < sig_basic_filtering[TTEETH]
    ].copy()

    #
    #sig_tmopen_in_tteeth
    sig_tmopen_in_tteeth = sig_basic_filtering[
        sig_basic_filtering[HIGH] < sig_basic_filtering[TTEETH]
        ].copy()

    sig_tmopen_in_tteeth = sig_tmopen_in_tteeth[  #@a The Big Mouth Is Open
        sig_tmopen_in_tteeth[TLIPS] > sig_tmopen_in_tteeth[TTEETH]
        ]
    sig_tmopen_in_tteeth = sig_tmopen_in_tteeth[  #@a The Big Mouth Is Open
        sig_tmopen_in_tteeth[TTEETH] > sig_tmopen_in_tteeth[TJAW]
        ]      
    
    #
    #sig_tmopen_in_tlips
    sig_tmopen_in_tlips = sig_basic_filtering[
        sig_basic_filtering[HIGH] > sig_basic_filtering[TLIPS]
        ].copy()

    # the BMouth is Openned
    sig_tmopen_in_tlips = sig_tmopen_in_tlips[
        sig_tmopen_in_tlips[TLIPS] > sig_tmopen_in_tlips[TTEETH]
        ]
    sig_tmopen_in_tlips = sig_tmopen_in_tlips[
        sig_tmopen_in_tlips[TTEETH] > sig_tmopen_in_tlips[TJAW]
        ]
    
    #






# INDEPENDENT OF DIRECTIONS
sig_basic_filtering_count = len(sig_basic_filtering)
sig_basic_filtering_sum=sig_basic_filtering[eval_target_colname].sum()

sig_nmopen_count = len(sig_nmopen)
sig_nmopen_sum=sig_nmopen[eval_target_colname].sum()





if not quiet:
    print("sig_basic_filtering:",sig_basic_filtering_count," sum:",sig_basic_filtering_sum)


# %%

if not quiet:
    print(sig_basic_filtering.tail(40))
    print("count3:",sig_nmopen_count," sum3:",sig_nmopen_sum)

# %%

if not quiet:
    print(sig_nmopen.tail(40))


# %%
if not quiet:
    print("count (no validation):",all_context_signal_count," sum0:",all_signalsnal_sum)
    print("count_2 sig_basic_filtering:",sig_basic_filtering_count," sum2:",sig_basic_filtering_sum)
    print("count_sell3 sig_nmopen:",sig_nmopen_count," sum3:",sig_nmopen_sum)






# # %%
# dfosell_vaoc_min_bar.tail(20)
# %%

# %%

result_of_this_analysis = """
Best Validation : not in lips and teeth

NEXT STEP : 

* Larger Timeframe insights.
"""


#@STCGoal  Eval Big Mouth 
# %%  BIG ALLIGATOR SELL - Eval Low is Bellow Big Teeth



sig_is_in_tteeth_count = len(sig_is_in_tteeth)
sig_is_in_tteeth_sum=sig_is_in_tteeth[eval_target_colname].sum()

# %%

if not quiet:
    print(sig_is_in_tteeth.tail(40))
    print("sig_is_in_tteeth:",sig_is_in_tteeth_count," sum:",sig_is_in_tteeth_sum)




# %%  BIG ALLIGATOR  - Eval Low is Above Big Lips and Big Mouth is Open OR RELATED tO BUY

                                       
sig_tmopen_in_tteeth_count = len(sig_tmopen_in_tteeth)
sig_tmopen_in_tteeth_sum=sig_tmopen_in_tteeth[eval_target_colname].sum()

# %%

if not quiet:
    print(sig_tmopen_in_tteeth.tail(40))
    print("count_sell2s2 Low is Above Big Teeth and Big Mouth is Open:",sig_tmopen_in_tteeth_count," sum2s2:",sig_tmopen_in_tteeth_sum)


# %%  BIG ALLIGATOR - Eval Low is Above Big Lips and Big Mouth is Open OR RELATED tO BUY

                                                                    
sig_tmopen_in_tlips_count = len(sig_tmopen_in_tlips)
sig_tmopen_in_tlips_sum=sig_tmopen_in_tlips[eval_target_colname].sum()

# %%
if not quiet:
    print(sig_tmopen_in_tlips.tail(40))
    print("count_sell2s2 Low is Above Big Lips and Big Mouth is Open:",sig_tmopen_in_tlips_count," sum2s2:",sig_tmopen_in_tlips_sum)





# %%


def write_to_result_csv(i,t,direction,nb_entry, tsum, title,_df=None):
    per_trade=round(tsum/nb_entry,2)
    if direction == "sell" or direction=="Sell" or direction=="SELL":direction="S"
    if direction == "buy" or direction=="Buy" or direction=="BUY":direction="B"
    
    #per_trade="per_trade"
    with open(result_file_csv, "a") as file_object:
        tsum_rounded = round(tsum,2)
        file_object.write(f"{i},{t},{direction},{per_trade},{nb_entry},{tsum_rounded},{title}\n")
    if _df is not None:
        save_df_archives(i, t, title, _df)

def save_df_archives(i, t, title, _df,quiet=True):
    ifn = i.replace("/","-")
    csv_fn = f"{source_dataset_archival_path}/{ifn}_{t}_{title}.csv"
    if not quiet:
        print("Writing CSV to file:",csv_fn)
    _df.to_csv(csv_fn)

#save the original df
if archive_used_dataset:
    save_df_archives(i, t, "original", df)
#write_to_result_csv("instrument","timeframe","nb_entry","tsum","title")

def write_to_result_md(entry_line):
    with open(result_file_md, "a") as file_object:
        file_object.write(f"{entry_line}\n")

def print_res(i,t,direction,nb_entry, tsum, title,_df=None):
    per_trade=round(tsum/nb_entry,2)
    tsum_rounded = round(tsum,2)
    entry_line = f"{i}_{t} {direction} pt:{per_trade} t:{nb_entry} sum:{tsum_rounded} title:{title}"
    print(entry_line)
    write_to_result_md(entry_line)
    write_to_result_csv(i,t,direction,nb_entry, tsum, title,_df)
# %%

# bellow all the output column names, in further development, we will want to have a more dynamic way to get the column names and evaluate both the Tide Alligator and Big ALligator and filter FDB signals with both of them, therefore, we will have a generic function that will create these column name according the which filtering is used
out_all_signal_sum_colname = "all_signals"
out_sig_in_normal_mouth_sum_colname = "sig_nmopen"
out_basic_normal_mouth_filtered_sum_colname = "sig_basic_filtering"

current_filtering_name = "Tide Alligator"

out_sig_is_in_context_teth_sum_colname = "sig_is_in_tteeth_sum"
out_sig_context_mouth_is_open_and_signal_is_in_context_lips_sum_colname = "sig_tmopen_in_tlips"
out_sig_context_mouth_is_open_and_signal_is_in_context_teeth_sum_colname = "sig_tmopen_in_tteeth_sum"


write_to_result_md("  ")
write_to_result_md("----")
write_to_result_md("  ")
write_to_result_md("==============================================================")

print_res(i,t,bs,all_context_signal_count,all_signalsnal_sum,out_all_signal_sum_colname,dfo_context)
print_res(i,t,bs,sig_nmopen_count,sig_nmopen_sum,out_sig_in_normal_mouth_sum_colname,sig_nmopen)
write_to_result_md("==============================================================")
print_res(i,t,bs,sig_basic_filtering_count,sig_basic_filtering_sum,out_basic_normal_mouth_filtered_sum_colname,sig_basic_filtering)
write_to_result_md("==============================================================")
print_res(i,t,bs,sig_is_in_tteeth_count,sig_is_in_tteeth_sum,out_sig_is_in_context_teth_sum_colname,sig_is_in_tteeth)
print_res(i,t,bs,sig_tmopen_in_tlips_count,sig_tmopen_in_tlips_sum,out_sig_context_mouth_is_open_and_signal_is_in_context_lips_sum_colname,sig_tmopen_in_tlips)
#pt:32.42 t:132 sum:4279.999999999999 title:count_sell2s2 Low is Above Big Lips and Big Mouth is Open
print_res(i,t,bs,sig_tmopen_in_tteeth_count,sig_tmopen_in_tteeth_sum,out_sig_context_mouth_is_open_and_signal_is_in_context_teeth_sum_colname,sig_tmopen_in_tteeth)
write_to_result_md("==============================================================")


#@STCIssue SPX500 D1, sig_tmopen_in_tteeth_sum has an interesting value when the Signal bar is in the big mouth
#pt:83.2 t:99 sum:8237.0 title:sig_tmopen_in_tteeth_sum
# %%
print("Saved md: ",result_file_md)
print("Saved csv: ",result_file_csv)
print("Completed processing for ",i,t,bs)