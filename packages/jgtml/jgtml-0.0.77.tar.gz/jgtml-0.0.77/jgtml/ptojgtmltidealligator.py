# This file is generated by pto-pre-build.sh. Do not edit it manually.
# Mon 24 Jun 2024 04:29:37 AM EDT
# SOURCE NAME: jgtml_obsds_240515_TIDE_SIGNALS.py
# ------------------------------------
#%% Imports 

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

import pandas as pd

CLI_DESCRIPTION = 'Process financial market data to analyze the performance of SELL signals generated by the "Fractal Divergent Bar" indicator within the "Tide Alligator" technical analysis tool.'

EPILOG="""
This Python script is designed to analyze the effectiveness of signals in the context of the "Tide Alligator" technical analysis tool, specifically those generated by the "Fractal Divergent Bar" indicator. The script reads historical market data for a specified instrument and timeframe, calculates technical indicators, and identifies valid SELL and BUY signals based on predefined criteria. 

The script then evaluates the profitability of each valid signal's direction, analyzes the distribution of profit potential across different signal types, and compares their performance. The results of the analysis are outputted as a CSV file, and a markdown file summarizing the key findings is also generated.
"""
X_ABSTRACT="""
# Abstract"""+EPILOG+"""



The script leverages the `jgtpy` and `jgtml` Python packages for data analysis and machine learning. It is an early prototype made during my coding mastery and is somehow customizable, allowing for the analysis of different instruments, timeframes, and signal types using the CLI. This script is part of a broader research project exploring the use of technical indicators in trading.

Future work includes further data analysis to identify additional patterns and insights, the development of a trading strategy based on the analysis findings. This script serves as a experiment to find valuable tool for improving trading strategies and decision-making by providing insights into the effectiveness of fractal divergent signals generated by the "Fractal Divergent Bar" indicator and their relationship with the "Tide Alligator" tool.

The 6 types of signals analyzed in this script are:

- all_signals
- sig_nmopen
- sig_basic_filtering
- sig_is_in_tteeth
- sig_tmopen_in_tteeth
- sig_tmopen_in_tlips

The script outputs the following metrics for each signal type:

- per_trade: Average profit per trade
- nb_entry: Number of valid signals for this type (could compare to the total number if signal of the "all_signals" column to learn how many signals are validated by this type of signal)
- tsum: Total profit across all signals
- title: Signal type

The script also generates a markdown file summarizing the key findings and a CSV file containing the detailed results of the analysis.

Details of each type of signals

- all_signals: All valid signals made by the jgtml.jtc.pto_target_calculation function
- sig_nmopen: Signals where the mouth of the "Regular Alligator" is open
- sig_basic_filtering: Signals where the price bar is out of the lips and teeth of the "Regular Alligator"
- sig_is_in_tteeth: Signals where the price bar has came back in the big teeth of the "Big Alligator" without the mouth being open or not.
- sig_tmopen_in_tteeth: Signals where the price bar has came back in the big teeth and the mouth of the "Big Alligator" is open (Exploring a Strategic entry when the mouth is open and we want to enter at the end of a retracement.  This explores signals that pulled back in the big teeth(the middest of the big balancing lines system))
- sig_tmopen_in_tlips: Signals where the price bar has came back in the big lips and the mouth of the "Big Alligator" is open (Exploring a Strategic entry when the mouth is open and we want to enter at the end of a retracement. This explores signals that pulled back in the big lips (the smallest of the big balancing lines system) )

Further analysis and interpretation of the results are needed to draw meaningful conclusions and insights from the data. The script is intended to be a starting point for more in-depth research and exploration of technical indicators in trading.

"""

import os

import numpy as np
import pandas as pd
try:
    from jgtpy import JGTCDS as cds
except:
    print("pip install -U jgtpy")

from jgtml import jtc

# Use jgtconstants column names from jgtutils
from jgtutils.jgtconstants import LOW,HIGH,FDBB,FDBS,BJAW,BLIPS,BTEETH,JAW,TEETH,LIPS,FDB_TARGET,VECTOR_AO_FDBS_COUNT,VECTOR_AO_FDBB_COUNT,VECTOR_AO_FDB_COUNT,TJAW,TLIPS,TTEETH
#FDB_TARGET=FDB_TARGET

def crop_dataframe(df, crop_last_dt: str = None, crop_start_dt: str = None):
    if crop_last_dt is not None:
        df = df[df.index <= crop_last_dt]
    if crop_start_dt is not None:
        df = df[df.index >= crop_start_dt]
    return df

#%% Contexts 
# Contexts

i = 'GBP/USD'
i = 'SPX500'
t = "H4"
t = "D1"

i=os.getenv("i",i)
t=os.getenv("t",t)

# FLAGS for the PROCESS we might want to configure or read from ENV
force_regenerate_mxfiles=False if os.getenv("force_regenerate_mxfiles") == "False" or os.getenv("force_regenerate_mxfiles") == "false" or os.getenv("force_regenerate_mxfiles") == "0" else True
mfi_flag=True
balligator_flag=True
talligator_flag=True
regenerate_cds=False if os.getenv("regenerate_cds") == "False" or os.getenv("regenerate_cds") == "false" or os.getenv("regenerate_cds") == "0" else True
use_fresh=False if os.getenv("use_fresh") == "False" or os.getenv("use_fresh") == "false" or os.getenv("use_fresh") == "0" else True
# use_fresh=True if os.getenv("use_fresh") == "True" or os.getenv("use_fresh") == "true" or os.getenv("use_fresh") == "1" else False
quiet=False if os.getenv("quiet") == "False" or os.getenv("quiet") == "false" or os.getenv("quiet") == "0" else True

# OUTPUTS Files
jgtdroot_default="/b/Dropbox/jgt" #$jgtdroot
jgtdroot=os.getenv("jgtdroot",jgtdroot_default)
#result_drop_base_default="/b/Dropbox/jgt/drop" #$jgtdroot
drop_subdir = "drop"
result_file_basename_default = os.getenv("result_file_basename_default") if os.getenv("result_file_basename_default") is not None else "jgtml_obsds_240515_SIGNALS.result"
result_file_basename=result_file_basename_default

# Columns to select
sel_columns_base=[HIGH,LOW,JAW,TEETH,LIPS]

sel_columns_tide_alligator=[TJAW,TTEETH,TLIPS]
sel_columns_big_alligator=[BJAW,BTEETH,BLIPS]

# Select columns according to Flags (big alligator and tide alligator)
sel_columns_common=sel_columns_base + \
    (sel_columns_big_alligator if balligator_flag else None) +  \
        (sel_columns_tide_alligator if talligator_flag else None) + \
            [FDB_TARGET,VECTOR_AO_FDB_COUNT]

sel_columns_sell = sel_columns_common + [FDBS, VECTOR_AO_FDBS_COUNT]
sel_columns_buy = sel_columns_common + [FDBB, VECTOR_AO_FDBB_COUNT]

# print(sel_columns_common)
# print(sel_columns_sell)
# print(sel_columns_buy)
#%%

# sel_columns_sell = [HIGH,LOW,BJAW,BLIPS,BTEETH,TJAW,TTEETH,TLIPS,JAW,TEETH,LIPS,FDBS,FDB_TARGET,VECTOR_AO_FDBS_COUNT,VECTOR_AO_FDB_COUNT]
# sel_columns_buy = [HIGH,LOW,BJAW,BLIPS,BTEETH,TJAW,TTEETH,TLIPS,JAW,TEETH,LIPS,FDBB,FDB_TARGET,VECTOR_AO_FDBB_COUNT,VECTOR_AO_FDB_COUNT]

jgtpy_data_full_var_name = "JGTPY_DATA_FULL"

bs="S" # This prototype is for SELL signals only, we can extend it to BUY signals later
bs=os.getenv("bs",bs)

result_drop_base_override=None
source_dataset_archival_path_override=None
data_dir_override=None
archive_used_dataset=True
use_ttf_default = True


def main():
    global i,t,bs,print_abstract,regenerate_cds,result_drop_base_override,source_dataset_archival_path_override,quiet,data_dir_override,use_fresh
    import argparse
    parser = argparse.ArgumentParser(description=CLI_DESCRIPTION,epilog=EPILOG)
    parser.add_argument('-i','--instrument', type=str, help='Instrument')
    parser.add_argument('-t','--timeframe', type=str, help='Timeframe')
    #bs
    parser.add_argument('-bs','--buysell', type=str, default='S', help='Buy or Sell')
    #regenerate_cds
    parser.add_argument('-nocds','--dont_regenerate_cds', help='Dont Regenerate CDS',action='store_true')
    #use_fresh
    parser.add_argument('-nf','--nofresh', help='Dont Use Fresh (Assuming you need CDS Already generated and available)',action='store_true')
    #parser.add_argument('-old','--not_fresh', help='Dont Use Fresh (Assuming you need CDS Already generated and available)',action='store_true')

    #data_dir_override
    parser.add_argument('-dd','--data_dir_override', type=str, default=None, help='Data Directory Override path (JGTPY_DATA_FULL env var will be used if not provided)')
    #result_drop_base_override
    parser.add_argument('-rdb','--result_drop_base_override', type=str, default=None, help='Result Drop Base Override')
    #source_dataset_archival_path_override
    parser.add_argument('-sda','--source_dataset_archival_path_override', type=str, default=None, help='Source Dataset Archival Path Override')
    # quiet
    parser.add_argument('-q','--quiet', help='Quiet',action='store_true')
    #print X_ABSTRACT
    parser.add_argument('-abstract','--print_abstract', action='store_true', help='Print Abstract')
    parser.add_argument('-ttf','--use_ttf_rather_than_cds', action='store_true', help='Force usage of TTF as source of data (CDS + HTF data)')
    #result_file_basename
    parser.add_argument('-rfb','--result_file_basename', type=str, default=result_file_basename_default, help='Result File Base Name')

    args = parser.parse_args()
    print_abstract = args.print_abstract
    if print_abstract:
        print(X_ABSTRACT,flush=True)
        exit(0)

    print(f"i:{i} t:{t} bs:{bs}")

    i = args.instrument
    t = args.timeframe
    bs = args.buysell
    regenerate_cds = not args.dont_regenerate_cds #default is to regenerate
    result_drop_base_override = args.result_drop_base_override
    source_dataset_archival_path_override = args.source_dataset_archival_path_override
    quiet = args.quiet
    data_dir_override = args.data_dir_override
    use_fresh= False if args.nofresh else True
    
    #@STCIssue We would do that by default
    use_ttf_rather_than_cds=args.use_ttf_rather_than_cds if args.use_ttf_rather_than_cds else False #@STCGoal Integration of all Signal Evaluation in one Script - We want HTF(ttf) data to be used in the evaluation so we can have a better understanding of the context of the signal with higher timeframes (Zone,MFI, etc)


NB_CONTEXT_RUN=False
def _chg_logics_for_notebook_context():
    global use_fresh, regenerate_cds
    #TODO Chg logics for notebook context
    regenerate_cds=False
    use_fresh=False

import IPython

if IPython.get_ipython() is not None:
    # Code is running in a Jupyter notebook
    print("Notebook context detected")
    NB_CONTEXT_RUN=True
else:
    # Code is running outside a Jupyter notebook
    print("Notebook context not detected")
    NB_CONTEXT_RUN=False

if not NB_CONTEXT_RUN:
    if __name__ == "__main__":
        main()
else:
    _chg_logics_for_notebook_context()

#%% 
print(f"i:{i} t:{t} bs:{bs}")
#%%

# Where to save the results
result_drop_base=os.path.join(jgtdroot, drop_subdir) if result_drop_base_override is None else result_drop_base_override

if not os.path.exists(result_drop_base):
    result_drop_base=os.path.join(".", drop_subdir)
    os.makedirs(result_drop_base,exist_ok=True)
    

#Where source dataset is archived 
source_dataset_archival_path= os.path.join(result_drop_base, "data", "arch", "jgtml_240516") if source_dataset_archival_path_override is None else source_dataset_archival_path_override

# Columns to select based on the direction    
if bs=="S" or bs=="SELL" or bs=="sell":
    sel_columns = sel_columns_sell
else:
    if bs=="B" or bs=="BUY" or bs=="buy":
        sel_columns = sel_columns_buy

result_file_base = os.path.join(result_drop_base, result_file_basename)

result_file_md=result_file_base + ".md"
result_file_csv=result_file_base + ".csv"


try:
    os.makedirs(source_dataset_archival_path,exist_ok=True)
except:
    #exit the app with error about the directory
    print(f"Error creating directory {source_dataset_archival_path}")
    exit(1)



ifn=i.replace("/","-")
data_dir = os.getenv(jgtpy_data_full_var_name) if data_dir_override is None else data_dir_override
#idsfilepath = os.path.join(data_dir, "targets", "mx", f"{ifn}_{t}.csv")
#df = pd.read_csv(idsfilepath)
df=None
try:
    if not force_regenerate_mxfiles:
        df = jtc.readMXFile(i,t)
except:
    pass

#%pip install -U jgtpy

#set DF to None if column 'mfi' is not present (force regeneration)
if df is not None and 'mfi' not in df.columns: #TMP to force regeneration if column mfi is not present
    df = None

if df is None:
    df, sel1, sel2 = jtc.pto_target_calculation(i,t,
                                                mfi_flag=mfi_flag,talligator_flag=talligator_flag,
                                                balligator_flag=balligator_flag,
                                                regenerate_cds=regenerate_cds,
                                                use_fresh=use_fresh,
                                                use_ttf=use_ttf_default)

#%%
if NB_CONTEXT_RUN:
    quiet=False if os.getenv("quiet") is None else quiet

if not quiet:
    l=len(df)
    print(df.tail(1))
    print(df.columns)
    print("len(df):",len(df))

#%% 
"""
'Volume', 'Open', HIGH, LOW, 'Close', 'Median', 'ao', 'ac', JAW,
       TEETH, LIPS, 'fh', 'fl', 'fh3', 'fl3', 'fh5', 'fl5', 'fh8', 'fl8',
       'fh13', 'fl13', 'fh21', 'fl21', 'fh34', 'fl34', 'fh55', 'fl55', 'fh89',
       'fl89', 'fdbb', 'fdbs', 'fdb', 'aoaz', 'aobz', 'zlc', 'zlcb', 'zlcs',
       'zcol', 'sz', 'bz', 'acs', 'acb', 'ss', 'sb', 'price_peak_above',
       'price_peak_bellow', 'ao_peak_above', 'ao_peak_bellow', 'tmax', 'tmin',
       'p', 'l', TARGET, 'vaos', 'vaob', 'vaosc', 'vaobc', 'vaoc'],
      
"""

# create a dataset with only the columns we need.  
dfo = df[sel_columns]


# %%
if not quiet:
    print(dfo.columns)

# %%
# Select row where target is not 0
dfo = dfo[dfo[FDB_TARGET] != 0] 
#dfoprofit = dfo[dfo[TARGET] > 0] 
# %%
if not quiet:
    print(dfo)


#%% Direction
direction = bs #Might use only one variable in the future to avoid confusion
# %%
dfo_context = dfo[sel_columns].copy()

#
fdb_context_colname = FDBS
if direction=="B":
    fdb_context_colname = FDBB
dfo_context = dfo_context[dfo_context[fdb_context_colname] != 0].copy()





# dfo = dfo[dfo[TARGET] != 0] #Where target is not 0
# %%
dfo_context.tail(40)
# %%
all_context_signal_count = len(dfo_context)

target_colname = FDB_TARGET
all_signalsnal_sum=dfo_context[target_colname].sum()

print("count:",all_context_signal_count," sum0:",all_signalsnal_sum)

# %%
#Remove invalid signal when column High < lips
#@STCGoal Valid Signals are bellow the lips and teeth 
if bs=="S":
    sig_basic_filtering = dfo_context[dfo_context[LOW] > dfo_context[LIPS]].copy()
    sig_basic_filtering = sig_basic_filtering[sig_basic_filtering[LOW] > sig_basic_filtering[TEETH]]

    #@STCGoal Valid Signals when mouth is open
    sig_nmopen = sig_basic_filtering[sig_basic_filtering[JAW] < sig_basic_filtering[TEETH]].copy()
    sig_nmopen = sig_nmopen[sig_nmopen[TEETH] < sig_nmopen[LIPS]]
    sig_nmopen = sig_nmopen[sig_nmopen[JAW] < sig_nmopen[LIPS]]

    #sig_is_in_tteeth
    sig_is_in_tteeth = sig_basic_filtering[
    sig_basic_filtering[LOW] > sig_basic_filtering[TTEETH]
    ].copy()

    #sig_tmopen_in_tteeth
    sig_tmopen_in_tteeth = sig_basic_filtering[
        sig_basic_filtering[LOW] > sig_basic_filtering[TTEETH]
        ].copy()

    sig_tmopen_in_tteeth = sig_tmopen_in_tteeth[  #@a The Big Mouth Is Open
        sig_tmopen_in_tteeth[TLIPS] < sig_tmopen_in_tteeth[TTEETH]
        ]
    sig_tmopen_in_tteeth = sig_tmopen_in_tteeth[  #@a The Big Mouth Is Open
        sig_tmopen_in_tteeth[TTEETH] < sig_tmopen_in_tteeth[TJAW]
        ]        

    #sig_tmopen_in_tlips
    sig_tmopen_in_tlips = sig_basic_filtering[
        sig_basic_filtering[LOW] < sig_basic_filtering[TLIPS]
        ].copy()

    # the BMouth is Openned
    sig_tmopen_in_tlips = sig_tmopen_in_tlips[
        sig_tmopen_in_tlips[TLIPS] < sig_tmopen_in_tlips[TTEETH]
        ]
    sig_tmopen_in_tlips = sig_tmopen_in_tlips[
        sig_tmopen_in_tlips[TTEETH] < sig_tmopen_in_tlips[TJAW]
        ]
    
    #
                    

else:
    sig_basic_filtering = dfo_context[dfo_context[HIGH] < dfo_context[LIPS]].copy()
    sig_basic_filtering = sig_basic_filtering[sig_basic_filtering[HIGH] < sig_basic_filtering[TEETH]]

    #sig_nmopen
    sig_nmopen = sig_basic_filtering[sig_basic_filtering[JAW] > sig_basic_filtering[TEETH]].copy()
    sig_nmopen = sig_nmopen[sig_nmopen[TEETH] > sig_nmopen[LIPS]]
    sig_nmopen = sig_nmopen[sig_nmopen[JAW] > sig_nmopen[LIPS]]

    #sig_is_in_tteeth
    sig_is_in_tteeth = sig_basic_filtering[
    sig_basic_filtering[HIGH] < sig_basic_filtering[TTEETH]
    ].copy()

    #
    #sig_tmopen_in_tteeth
    sig_tmopen_in_tteeth = sig_basic_filtering[
        sig_basic_filtering[HIGH] < sig_basic_filtering[TTEETH]
        ].copy()

    sig_tmopen_in_tteeth = sig_tmopen_in_tteeth[  #@a The Big Mouth Is Open
        sig_tmopen_in_tteeth[TLIPS] > sig_tmopen_in_tteeth[TTEETH]
        ]
    sig_tmopen_in_tteeth = sig_tmopen_in_tteeth[  #@a The Big Mouth Is Open
        sig_tmopen_in_tteeth[TTEETH] > sig_tmopen_in_tteeth[TJAW]
        ]      
    
    #
    #sig_tmopen_in_tlips
    sig_tmopen_in_tlips = sig_basic_filtering[
        sig_basic_filtering[HIGH] > sig_basic_filtering[TLIPS]
        ].copy()

    # the BMouth is Openned
    sig_tmopen_in_tlips = sig_tmopen_in_tlips[
        sig_tmopen_in_tlips[TLIPS] > sig_tmopen_in_tlips[TTEETH]
        ]
    sig_tmopen_in_tlips = sig_tmopen_in_tlips[
        sig_tmopen_in_tlips[TTEETH] > sig_tmopen_in_tlips[TJAW]
        ]
    
    #






# INDEPENDENT OF DIRECTIONS
sig_basic_filtering_count = len(sig_basic_filtering)
sig_basic_filtering_sum=sig_basic_filtering[FDB_TARGET].sum()

sig_nmopen_count = len(sig_nmopen)
sig_nmopen_sum=sig_nmopen[FDB_TARGET].sum()





if not quiet:
    print("sig_basic_filtering:",sig_basic_filtering_count," sum:",sig_basic_filtering_sum)


# %%

if not quiet:
    print(sig_basic_filtering.tail(40))
    print("count3:",sig_nmopen_count," sum3:",sig_nmopen_sum)

# %%

if not quiet:
    print(sig_nmopen.tail(40))


# %%
if not quiet:
    print("count (no validation):",all_context_signal_count," sum0:",all_signalsnal_sum)
    print("count_2 sig_basic_filtering:",sig_basic_filtering_count," sum2:",sig_basic_filtering_sum)
    print("count_sell3 sig_nmopen:",sig_nmopen_count," sum3:",sig_nmopen_sum)






# # %%
# dfosell_vaoc_min_bar.tail(20)
# %%

# %%

result_of_this_analysis = """
Best Validation : not in lips and teeth

NEXT STEP : 

* Larger Timeframe insights.
"""


#@STCGoal  Eval Big Mouth 
# %%  BIG ALLIGATOR SELL - Eval Low is Bellow Big Teeth



sig_is_in_tteeth_count = len(sig_is_in_tteeth)
sig_is_in_tteeth_sum=sig_is_in_tteeth[FDB_TARGET].sum()

# %%

if not quiet:
    print(sig_is_in_tteeth.tail(40))
    print("sig_is_in_tteeth:",sig_is_in_tteeth_count," sum:",sig_is_in_tteeth_sum)




# %%  BIG ALLIGATOR  - Eval Low is Above Big Lips and Big Mouth is Open OR RELATED tO BUY

                                       
sig_tmopen_in_tteeth_count = len(sig_tmopen_in_tteeth)
sig_tmopen_in_tteeth_sum=sig_tmopen_in_tteeth[FDB_TARGET].sum()

# %%

if not quiet:
    print(sig_tmopen_in_tteeth.tail(40))
    print("count_sell2s2 Low is Above Big Teeth and Big Mouth is Open:",sig_tmopen_in_tteeth_count," sum2s2:",sig_tmopen_in_tteeth_sum)


# %%  BIG ALLIGATOR - Eval Low is Above Big Lips and Big Mouth is Open OR RELATED tO BUY

                                                                    
sig_tmopen_in_tlips_count = len(sig_tmopen_in_tlips)
sig_tmopen_in_tlips_sum=sig_tmopen_in_tlips[FDB_TARGET].sum()

# %%
if not quiet:
    print(sig_tmopen_in_tlips.tail(40))
    print("count_sell2s2 Low is Above Big Lips and Big Mouth is Open:",sig_tmopen_in_tlips_count," sum2s2:",sig_tmopen_in_tlips_sum)





# %%


def write_to_result_csv(i,t,direction,nb_entry, tsum, title,_df=None):
    per_trade=round(tsum/nb_entry,2)
    if direction == "sell" or direction=="Sell" or direction=="SELL":direction="S"
    if direction == "buy" or direction=="Buy" or direction=="BUY":direction="B"
    
    #per_trade="per_trade"
    with open(result_file_csv, "a") as file_object:
        tsum_rounded = round(tsum,2)
        file_object.write(f"{i},{t},{direction},{per_trade},{nb_entry},{tsum_rounded},{title}\n")
    if _df is not None:
        save_df_archives(i, t, title, _df)

def save_df_archives(i, t, title, _df,quiet=True):
    ifn = i.replace("/","-")
    csv_fn = f"{source_dataset_archival_path}/{ifn}_{t}_{title}.csv"
    if not quiet:
        print("Writing CSV to file:",csv_fn)
    _df.to_csv(csv_fn)

#save the original df
if archive_used_dataset:
    save_df_archives(i, t, "original", df)
#write_to_result_csv("instrument","timeframe","nb_entry","tsum","title")

def write_to_result_md(entry_line):
    with open(result_file_md, "a") as file_object:
        file_object.write(f"{entry_line}\n")

def print_res(i,t,direction,nb_entry, tsum, title,_df=None):
    per_trade=round(tsum/nb_entry,2)
    tsum_rounded = round(tsum,2)
    entry_line = f"{i}_{t} {direction} pt:{per_trade} t:{nb_entry} sum:{tsum_rounded} title:{title}"
    print(entry_line)
    write_to_result_md(entry_line)
    write_to_result_csv(i,t,direction,nb_entry, tsum, title,_df)
# %%

write_to_result_md("  ")
write_to_result_md("----")
write_to_result_md("  ")
write_to_result_md("==============================================================")
print_res(i,t,direction,all_context_signal_count,all_signalsnal_sum,"all_signals",dfo_context)
print_res(i,t,direction,sig_nmopen_count,sig_nmopen_sum,"sig_nmopen",sig_nmopen)
write_to_result_md("==============================================================")
print_res(i,t,direction,sig_basic_filtering_count,sig_basic_filtering_sum,"sig_basic_filtering",sig_basic_filtering)
write_to_result_md("==============================================================")
print_res(i,t,direction,sig_is_in_tteeth_count,sig_is_in_tteeth_sum,"sig_is_in_tteeth_sum",sig_is_in_tteeth)
print_res(i,t,direction,sig_tmopen_in_tlips_count,sig_tmopen_in_tlips_sum,"sig_tmopen_in_tlips",sig_tmopen_in_tlips)
#pt:32.42 t:132 sum:4279.999999999999 title:count_sell2s2 Low is Above Big Lips and Big Mouth is Open
print_res(i,t,direction,sig_tmopen_in_tteeth_count,sig_tmopen_in_tteeth_sum,"sig_tmopen_in_tteeth_sum",sig_tmopen_in_tteeth)
write_to_result_md("==============================================================")


#@STCIssue SPX500 D1, sig_tmopen_in_tteeth_sum has an interesting value when the Signal bar is in the big mouth
#pt:83.2 t:99 sum:8237.0 title:sig_tmopen_in_tteeth_sum
# %%